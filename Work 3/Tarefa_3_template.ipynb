{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0884a9f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "799d1afd-bbf2-48d1-93be-79fbbdd43547",
   "metadata": {},
   "source": [
    "# Tarefa 3 - Neural Networks\n",
    "Third assessed coursework for the course: Técnicas e Algoritmos em Ciência de Dados\n",
    "\n",
    "This tarefa provides an exciting opportunity for students to put their knowledge acquired in class into practice, using neural networks to solve real-world problems in both classification and regression. Students will apply the concepts they have learned to build, train, and optimize neural networks, using a validation set to fine-tune hyperparameters. Students will also get used to generating important plots during training to analyse the models' behaviour. By the end of the project, students will have gained hands-on experience in implementing neural networks.\n",
    "\n",
    "## General guidelines:\n",
    "\n",
    "* This work must be entirely original. You are allowed to research documentation for specific libraries, but copying solutions from the internet or your classmates is strictly prohibited. Any such actions will result in a deduction of points for the coursework.\n",
    "* Please enter your code in the designated areas of the notebook. You can create additional code cells to experiment with, but __make sure to place your final solutions where they are requested in the notebook.__\n",
    "* Before submitting your work, make sure to rename the file to the random number that you created for the previous coursework (for example, 289479.ipynb).3\n",
    "\n",
    "## Notebook Overview:\n",
    "\n",
    "1. [Regression](#Regression) (50%)\n",
    "2. [Classification](#Classification) (50%)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1cd2f07-f367-4318-96f4-56d85d13c8f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Regression\n",
    "## Dataset and Problem Description\n",
    "In this exercise, you will use the Energy Efficiency Prediction dataset. This dataset contains information about the energy efficiency of buildings based on eight features, including the size of the building, the orientation, and the type of building materials used. The dataset includes two targets: heating load and cooling load, which represent the energy required to heat and cool the building, respectively.\n",
    "This dataset is useful for building neural networks that predict the energy efficiency of buildings, which is an important problem in the field of sustainable energy. The dataset has been used in several machine learning research papers and provides a challenging regression problem.\n",
    "\n",
    "## Exercise Description: Energy Efficiency Prediction with Neural Networks\n",
    "In this exercise, you will use the Energy Efficiency Prediction dataset provided.\n",
    "You will build and train a neural network to predict the heating load (column labelled y1 in the dataset) and the cooling load (column labelled y2) of a building based on its energy efficiency features. \n",
    "\n",
    "**To complete this exercise, you will write code to build and train neural networks for this problem:**\n",
    "\n",
    "1. Split the dataset into training, validation, and test sets, using a 70:15:15 ratio.\n",
    "\n",
    "2. Using numpy, build a neural network that takes in the energy efficiency features as input and predicts the heating load as output. You will choose the number of neurons per layers and the number of layers, but each layer will have the same number of neurons.\n",
    "\n",
    "3. Code the forward pass and backpropagation algorithm to learn the weights of the neural network. Use the training set to train the neural network and update the weights using stochastic gradient descent. You will need to regularize your neural network using weight decay, that is, you will include a regularization term in your error function.\n",
    "\n",
    "4. Monitor the training by plotting the training and validation losses across the epochs. \n",
    "\n",
    "\n",
    "The performance of your neural network will be different depending on the number of layers, number of neurons per layer and the value of λ that controls the amount of weight decay. You will experiment with 3 values of λ: 0 (no weight decay), 0.1 and 0.01.\n",
    "To choose the best network configuration and assess its performance you will:\n",
    "\n",
    "1. Calculate the loss for each configuration on the validation set.\n",
    "\n",
    "2. Generate 3 [heatmaps](https://seaborn.pydata.org/generated/seaborn.heatmap.html) one for each value of the λ regularization parameter, displaying the loss on the validation set by plotting the number of layers and number of neurons in a grid. This will help you visualise the best configuration for the neural network.\n",
    "\n",
    "3. Train your final model selecting the best combination of hyper-parameters and evaluate the final performance of the neural network using the test set and the root mean squared error as the metric and report that.\n",
    "\n",
    "**Important:**\n",
    "* Train for 50 epochs, remember that one epoch finishes when the whole training set was seen during training.\n",
    "* Set the learning rate $\\eta$ to $0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "186fb6bf-419a-4895-b9df-b31b5278b3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"## your code goes here:\\n\\n#Carregando o código:\\nEnergyEficciencydf = pd.read_csv('energy_efficiency.csv')\\nEnergyEficciencydf.head(-1)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''## your code goes here:\n",
    "\n",
    "#Carregando o código:\n",
    "EnergyEficciencydf = pd.read_csv('energy_efficiency.csv')\n",
    "EnergyEficciencydf.head(-1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f26c0fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Fazendo o Split do conjunto de dados.\\n\\nrandomstate = np.random.RandomState(13)\\n\\n\\nx = EnergyEficciencydf.iloc[:, 0:8]\\ny = EnergyEficciencydf.iloc[:, 8:10]\\n\\n#O primeiro split separa o treino com proporção de 70%\\nx_train, x_validation_test, y_train, y_validation_test = train_test_split(x,y, test_size=0.3, random_state=randomstate)\\n#O segundo spplit separa o teste e a validação ambos com proporção de 15%\\nx_validation, x_test, y_validation, y_test = train_test_split(x_validation_test, y_validation_test, test_size=0.5, random_state=randomstate)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#Fazendo o Split do conjunto de dados.\n",
    "\n",
    "randomstate = np.random.RandomState(13)\n",
    "\n",
    "\n",
    "x = EnergyEficciencydf.iloc[:, 0:8]\n",
    "y = EnergyEficciencydf.iloc[:, 8:10]\n",
    "\n",
    "#O primeiro split separa o treino com proporção de 70%\n",
    "x_train, x_validation_test, y_train, y_validation_test = train_test_split(x,y, test_size=0.3, random_state=randomstate)\n",
    "#O segundo spplit separa o teste e a validação ambos com proporção de 15%\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_test, y_validation_test, test_size=0.5, random_state=randomstate)'''\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93de5467-3e3c-45d4-bf03-04baeaf20193",
   "metadata": {},
   "source": [
    "# Classification\n",
    "## Dataset description: \n",
    "This is a dataset from the medical domain. It describes the problem of diagnosing coronary heart disease (CHD ) via Traditional Chinese Medicine approaches. Each datapoint corresponds to a patient represented by a set of 49 features corresponding to the presence or absence of different symptoms: feelings cold or warm, sweating, etc. The 6 labels represent presence or absence of specific heart conditions: deficiency of heart qi syndrome, deficiency of heart yang syndrome, deficiency of heart yin syndrome, qi stagnation syndrome, turbid phlegm syndrome, and blood stasis syndrome.\n",
    "\n",
    "## Exercise Description: CHD49 Multi-Label Classification with Neural Networks\n",
    "In this exercise, you will build and train a neural network to predict the 6 different labels of CHD (last 6 columns of the dataset). \n",
    "\n",
    "**To complete this exercise, follow these steps:**\n",
    "\n",
    "1. Load the dataset and split it into training, validation, and test sets, using a 70:15:15 ratio. \n",
    "\n",
    "2. Build a neural network using numpy that takes in the features as input and predicts the 6 different labels. You will choose the number of neurons per layers and the number of layers, but each layer will have the same number of neurons.\n",
    "\n",
    "3. Code the forward pass and backpropagation algorithm to learn the weights of the neural network. Use the training set to train the neural network and update the weights using batch gradient descent. You will choose the number of neurons per layers and the number of layers, but each layer will have the same number of neurons.\n",
    "\n",
    "4. Monitor the training by plotting the training and validation losses across the epochs. \n",
    "\n",
    "The performance of your neural network will be different depending on the number of layers, number of neurons per layer and the value of λ that controls the amount of weight decay. You will experiment with 3 values of λ: 0 (no weight decay), 0.1 and 0.01.\n",
    "To choose the best network configuration and assess its performance you will:\n",
    "\n",
    "1. Calculate the loss for each configuration on the validation set.\n",
    "\n",
    "2. Generate 3 heatmaps, one for each value of the λ regularization parameter, displaying the loss on the validation set by plotting the number of layers and number of neurons in a grid. This will help you visualise the best configuration for the neural network.\n",
    "\n",
    "3. Train your final model selecting the best combination of hyper-parameters and evaluate the final performance of the neural network using the test set and by calculating the area under the ROC curve, accuracy and F1 score as metrics and report these.\n",
    "\n",
    "**Important:**\n",
    "* Train for at least 1000 epochs, remember that one epoch finishes when the whole training set was seen during training.\n",
    "* Set the learning rate $\\eta$ to $0.01$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1256e9d5-779c-4a34-96ca-44275e9efc3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "      <th>att5</th>\n",
       "      <th>att6</th>\n",
       "      <th>att7</th>\n",
       "      <th>att8</th>\n",
       "      <th>att9</th>\n",
       "      <th>att10</th>\n",
       "      <th>...</th>\n",
       "      <th>att46</th>\n",
       "      <th>att47</th>\n",
       "      <th>att48</th>\n",
       "      <th>att49</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>label3</th>\n",
       "      <th>label4</th>\n",
       "      <th>label5</th>\n",
       "      <th>label6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>554 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     att1  att2  att3  att4  att5  att6  att7  att8  att9  att10  ...  att46  \\\n",
       "0     1.0  -1.0   1.0  -1.0   1.0   1.0   0.0  -0.5   1.0    1.0  ...    1.0   \n",
       "1    -1.0  -1.0  -1.0  -1.0   1.0   1.0  -1.0  -1.0  -1.0   -1.0  ...    1.0   \n",
       "2    -1.0   1.0  -1.0  -1.0   1.0  -1.0  -1.0  -1.0  -1.0   -1.0  ...   -1.0   \n",
       "3     1.0   1.0  -1.0   1.0  -1.0   1.0   0.0  -0.5   1.0    1.0  ...   -1.0   \n",
       "4    -1.0  -1.0  -1.0   1.0   1.0   1.0   0.0  -0.5  -1.0    1.0  ...   -1.0   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...    ...  ...    ...   \n",
       "549   1.0   1.0  -1.0  -1.0  -1.0   1.0  -1.0  -1.0  -1.0   -1.0  ...   -1.0   \n",
       "550   1.0  -1.0  -1.0  -1.0   1.0  -1.0   0.0   0.0   1.0   -1.0  ...   -1.0   \n",
       "551  -1.0  -1.0  -1.0  -1.0   1.0   1.0  -1.0  -1.0  -1.0   -1.0  ...   -1.0   \n",
       "552  -1.0  -1.0  -1.0   1.0   1.0   1.0  -1.0  -1.0  -1.0   -1.0  ...   -1.0   \n",
       "553   1.0   1.0  -1.0  -1.0   1.0   1.0   0.0   0.5   1.0    1.0  ...   -1.0   \n",
       "\n",
       "     att47  att48  att49  label1  label2  label3  label4  label5  label6  \n",
       "0      1.0    1.0    1.0     1.0     0.0     1.0     0.0     1.0     1.0  \n",
       "1      1.0    1.0   -1.0     1.0     0.0     0.0     1.0     0.0     0.0  \n",
       "2      1.0   -1.0    1.0     0.0     1.0     0.0     0.0     0.0     1.0  \n",
       "3      1.0    1.0   -1.0     0.0     1.0     0.0     0.0     0.0     1.0  \n",
       "4     -1.0   -1.0   -1.0     0.0     0.0     1.0     0.0     0.0     1.0  \n",
       "..     ...    ...    ...     ...     ...     ...     ...     ...     ...  \n",
       "549    1.0    1.0   -1.0     1.0     0.0     1.0     0.0     0.0     1.0  \n",
       "550   -1.0   -1.0   -1.0     1.0     0.0     0.0     0.0     0.0     1.0  \n",
       "551   -1.0   -1.0   -1.0     1.0     0.0     0.0     0.0     0.0     1.0  \n",
       "552    1.0   -1.0    1.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "553    1.0   -1.0    1.0     1.0     0.0     1.0     0.0     0.0     1.0  \n",
       "\n",
       "[554 rows x 55 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## your code goes here:\n",
    "\n",
    "#Carregando o código:\n",
    "CHDdf = pd.read_csv('CHD_49.csv')\n",
    "CHDdf.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "618f8aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fazendo o Split do conjunto de dados.\n",
    "\n",
    "randomstate = np.random.RandomState(13)\n",
    "\n",
    "\n",
    "x = CHDdf.iloc[:, 0:49]\n",
    "y = CHDdf.iloc[:, 49:55]\n",
    "\n",
    "#O primeiro split separa o treino com proporção de 70%\n",
    "x_train, x_validation_test, y_train, y_validation_test = train_test_split(x,y, test_size=0.3, random_state=randomstate)\n",
    "#O segundo spplit separa o teste e a validação ambos com proporção de 15%\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_test, y_validation_test, test_size=0.5, random_state=randomstate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebb3eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicializando os nossos parâmetros de pesos e vieses\n",
    "\n",
    "def initialize_weights(layers):\n",
    "    #A seed será a mesma usada na atividade passada\n",
    "    np.random.seed(13)\n",
    "    weights = []\n",
    "    biases = []\n",
    "    \n",
    "    for i in range(1, len(layers)):\n",
    "        weights.append(np.random.randn(layers[i-1], layers[i]))\n",
    "        biases.append(np.zeros((1, layers[i])))\n",
    "    return weights, biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14d38493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a Foward Propagation usando a função de ativação sigmoid\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "# Derivada da função de ativação sigmoid\n",
    "def sigmoid_derivative(Z):\n",
    "    return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "\n",
    "def forward_propagation(X, weights, biases):\n",
    "    #Iniciaremos com a primeira camada da nossa rede com os inputs e a cada camada será aplicada a sigmoid e retornaremos a lista nlayers que contém como último item o output layer\n",
    "    nlayers = [X]\n",
    "    for i in range(len(weights)):\n",
    "\n",
    "        layer = sigmoid(np.dot(nlayers[-1], weights[i]) + biases[i])\n",
    "        nlayers.append(layer)\n",
    "    return nlayers\n",
    "\n",
    "def rsme(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def back_propagation(y, layers, weights):\n",
    "    \n",
    "    output_error = y - layers[-1]\n",
    "    delta = output_error * sigmoid_derivative(layers[-1])\n",
    "\n",
    "    deltas = []\n",
    "    errors = []\n",
    "\n",
    "    errors.insert(0, output_error)\n",
    "    deltas.insert(0, delta)\n",
    "\n",
    "    i = len(layers) - 1\n",
    "    while i >= 0:\n",
    "        hidden_error = np.dot(delta, weights[i+1].T)\n",
    "        delta = hidden_error * sigmoid_derivative(layers[i])\n",
    "\n",
    "        errors.insert(0, hidden_error)\n",
    "        deltas.insert(0, delta)\n",
    "        \n",
    "        i -= 1\n",
    "\n",
    "    return errors, deltas, rsme(y, layers[-1])\n",
    "\n",
    "def update_parameters(weights, biases, deltas, layers, learning_rate):\n",
    "    i=0\n",
    "    j=0\n",
    "    \n",
    "    while i <= len(weights):\n",
    "        weights[i] += np.dot(layers[i], deltas[i]) * learning_rate\n",
    "        i += 1\n",
    "    \n",
    "    while j <= len(biases):\n",
    "        biases[j] += learning_rate * np.sum(deltas[j], axis=0)\n",
    "        j +=1\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98c55415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, epochs, learning_rate, hidden_neurons):\n",
    "    xshape = x.shape[1]\n",
    "    yshape = y.shape[1]\n",
    "    w1, w2, w3, b1, b2, b3 = initialize_weights(xshape, yshape, hidden_neurons)\n",
    "    for i in range(epochs):\n",
    "        Z1, Z2, Z3, A1, A2, output_layer = forward_propagation(x, w1, w2, w3, b1, b2, b3)\n",
    "\n",
    "        w1, w2, w3, b1, b2, b3, output_layer, output_error = back_propagation(x, y, output_layer, w3, w2, w1, A2, A1, b3, b2, b1, learning_rate)\n",
    "\n",
    "    return w1, w2, w3, b1, b2, b3\n",
    "\n",
    "def predict(X, w1, w2, w3, b1, b2, b3):\n",
    "    Z1, Z2, Z3, A1, A2, output_layer = forward_propagation(X, w1, w2, w3, b1, b2, b3)\n",
    "    return output_layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
